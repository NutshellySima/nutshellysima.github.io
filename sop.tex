\documentclass[12pt]{article}
\usepackage[
  a4paper,
  margin=1in,
  headsep=18pt, % separation between header rule and text
]{geometry}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{microtype}

\usepackage{url}

\newcommand{\statement}[1]{\medskip\noindent
  \textcolor{black}{\textbf{#1}}\space
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{
\textsc{\soptitle}\hfill\footnotesize
\textbf{\yourname}\\
\school\hfill\yourintent}
\fancyfoot[C]{\footnotesize\thepage}

\let\oldcenter\center
\let\oldendcenter\endcenter
\renewenvironment{center}{\setlength\topsep{0pt}\oldcenter}{\oldendcenter}

\newcommand{\wordcount}{
\begin{center}
\rule{0.75\textwidth}{.4pt}\\
{\footnotesize A statement of \numwords \ words}
\end{center}
}

\newif\ifcomments

\newcommand{\comment}[1]{}
\newcommand{\todo}[1]{\ifcomments{999}\fi}

%%%%%%%%%%%%%%%%%%%%% Edit this section %%%%%%%%%%%%%%%%%%%%

\commentstrue

\newcommand{\soptitle}{Statement of Purpose}
\newcommand{\yourname}{Chijun Sima}
\newcommand{\yourintent}{Applicant for Computer Science PhD}
\newcommand{\school}{The University of Edinburgh}
\newcommand{\advisorfirstname}{FirstName}
\newcommand{\advisorlastname}{LastName}
\newcommand{\advisorfull}{Prof. \advisorfirstname \ \advisorlastname}
\newcommand{\advisor}{Prof. \advisorlastname}
\newcommand{\numwords}{\todo{number}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[
  breaklinks,
  pdftitle={\yourname - \soptitle},
  pdfauthor={\yourname},
  unicode,
  colorlinks,
  urlcolor={blue!80!black}
]{hyperref}

\begin{document}

Large-scale AI models have advanced rapidly through scaling and reinforcement learning, but they expose challenges in training and serving. For the past five years, I have worked in the industry on building efficient machine learning systems to support model scaling and have published part of this work as a co-first author on the \textbf{OSDI '22}\cite{280902} paper. My industry experience gives me the intuition to identify the novel research topics that the industry cares about most. These experiences motivate my research goal: to co-design ML algorithms and systems that make large-scale, efficient learning both technically and economically feasible. I am applying to The University of Edinburgh CS PhD program to pursue this agenda at the intersection of \textbf{systems} and \textbf{machine learning}.

\statement{Background} Since 2020, I have worked at Tencent on efficient machine learning systems for WeChat, a mobile app with over a billion daily users.

\statement{Scaling up deep learning recommender systems (Ekko).} My first research focus was on infrastructure for scaling up our Deep Learning Recommender Models (DLRMs). In light of the discovery of the neural scaling law in 2020, we decided to scale up the DLRMs. Although scaling improved offline metrics, it severely harmed user engagement metrics on initial attempts. I traced this discrepancy to increased model-update latency: pre-scaling-era infrastructure could not propagate updates fast enough, causing the online system to serve stale models. I saw this as a fundamental systems challenge worth solving, and I took ownership of investigating the problem myself. This led me to formulate the research question: \textbf{how can we maintain low model-update latency as DLRMs scale to trillions of parameters?}

I led this project under the supervision of \href{https://luomai.github.io/}{Prof. Luo Mai}. I observed that the key barrier to achieving this objective in previous systems was the multi-step process they follow. Thus, I proposed disseminating updates directly, bypassing these steps. Then I identified several challenges: (1) High network costs - more than ten thousand models, each up to 10 TB, produce updates that need to be disseminated through expensive WANs. (2) High storage costs - Highly dynamic traffic leads to resource waste. (3) SLO risk - harmful updates can be deployed. For the first challenge, I designed a model update dissemination algorithm that compresses updates and an accuracy-aware update scheduler prioritizing updates based on gradients and model details, reducing bandwidth usage by \textbf{92\%} and using about \textbf{3\%} of total network traffic for synchronization. For the second challenge, I developed an SLO-aware shard manager using mathematical optimization to co-locate models without burdening inference engines, reducing cost by \textbf{49\%}. For the third challenge, I designed a model-state manager that rolls back deployments in seconds.

This project removed a key barrier to scaling up DLRMs. After deployment, my collaborators and I \textbf{scaled the model size by 10,000x}, increasing it from a few gigabytes to tens of terabytes per model, while reducing model update latency to \textbf{2.4s}. I presented the core ideas above and the immediate 100x scale-up in \textbf{Ekko (OSDI '22, co-first author)}\cite{280902}, the first work to demonstrate that careful co‑design of model deployment systems with model-aware mechanisms can make second‑level model freshness feasible even for multi‑terabyte DLRMs. After we deployed Ekko to scale DLRMs, in the six months following rollout, WeChat Channels' DAU and VV grew by \textbf{40\%} and \textbf{87\%}; although many factors contributed, Ekko was instrumental in enabling this growth\cite{wx}. Retrospective A/B tests also confirmed that delaying model updates in just part of the multi-stage pipeline could harm user engagement metrics by \textbf{1\%-3\%} over 15 days, implying that Ekko led to significant SLO improvements when used across the whole pipeline\cite{280902}. The same design has driven another 100x scale-up in production over the following year, and its second-level latency remains faster than publicly reported minute‑level latencies at comparable scale in 2025\cite{atscale}. Ekko currently serves \textbf{over a billion users daily.}

\statement{Scaling up the data engineering platform.} My second research focus has been on scaling up model training data at low cost. Following the trend of data scaling, Modern feature pipelines are long and increasingly multimodal, encouraging engineers to move away from highly optimized DSLs\cite{10.1145/3677998.3678220} and to use expressive, unsafe programming languages to construct composable operators that communicate across process boundaries, leading to the notoriously expensive "data center tax"\cite{44271}. To address this, I designed a system that (1) uses WebAssembly to provide in-process isolation, eliminating unnecessary process boundaries while preserving safety and resource constraints, and (2) reduces data transfers by placing operators near the data. This system is now widely used in WeChat and reduces data movement up to 1,200x on representative workloads. This project reinforced for me that building efficient large‑scale ML requires optimizing the entire pipeline, not just the core training loop.

\statement{Future research directions} Based on these experiences, I want to study how to make large foundation models continuously adaptive and cost-effective at scale. I am interested in (1) new algorithms and systems that incorporate user preference and knowledge promptly, e.g., (a) Coding assistants start to call on the development of low-latency LLM online RL systems\cite{cursor_rl}. In contrast, current industry systems suffer from a 1.5-2-hour delay. (b) I am also researching building an efficient LLM knowledge delivery network\cite{cheng2024largelanguagemodelsneed}. I hypothesize that remote KVCache can be gradually refined from a workload-aware compressed version. This idea has similarities to DLRM update prioritization, and Ekko's dissemination algorithm can also be repurposed to make different DCs get different compressed versions depending on time constraints and bandwidth; (2) workload-aware systems enabling further scaling by leveraging characteristics in user usage\cite{du2025prefillonlyinferenceengineprefillonly} and leveraging characteristics of model internals (e.g., the reasoning process\cite{pan2025specreasonfastaccurateinferencetime}, efficient sampling\cite{xie2024sanaefficienthighresolutionimage}); (3) ML systems that leverage LLMs to improve their efficiency\cite{cheng2025barbariansgateaiupending}, for example, using them to discover new quantization methods or kernels\cite{chen2025cudallmllmswriteefficient} or compute graph searching methods.

\statement{Why The University of Edinburgh?} The University of Edinburgh has a proven record of pioneering research in the foundation model era, spanning systems and algorithms to create efficient AI and tools that lower the barrier to using AI. So it is an ideal place to continue my research on co-designing ML algorithms and systems that will later have a chance to enter open-source ecosystems. There are several faculty members at The University of Edinburgh I'd be excited to work with, specifically Professor Luo Mai. I am interested in all sorts of measures to make AI efficient.

\statement{Career goals} My long-term career goal is to pursue both a professorship and entrepreneurship, as they offer complementary perspectives for understanding real-world problems and translating research into practice. I hope to make powerful AI technologies accessible to more people and organizations worldwide.

\wordcount

\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file

\end{document}
